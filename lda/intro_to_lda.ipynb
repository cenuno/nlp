{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlent Allocation (LDA)\n",
    "\n",
    "Made by: Cristian E. Nuno\n",
    "\n",
    "Date: June 16, 2019\n",
    "\n",
    "---------\n",
    "\n",
    "This tutorial follows the [Beginners Guide to Topic Modeling in Python](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "> In [natural language processing (NLP)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168328/), [latent Dirichlet allocation (LDA)](http://jmlr.csail.mit.edu/papers/v3/blei03a.html) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. [LDA](https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation) is an example of a [topic model](https://cacm.acm.org/magazines/2012/4/147361-probabilistic-topic-models/fulltext#F3). - Wikipedia ([source](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation))\n",
    "\n",
    "*Note: Hyperlinks are my own.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary libraries and modules\n",
    "\n",
    "The [`gensim`](https://radimrehurek.com/gensim/) library is used for topic modeling. The [`nltk` library](https://www.nltk.org/) is one the popular Python packages used for working with text data. The `string` library is imported to make use of its [constants](https://docs.python.org/3.4/library/string.html?highlight=string%20module#string-constants).\n",
    "\n",
    "<!-- Finally, the [`List`](https://docs.python.org/3/library/typing.html) module is useful when writing functions. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "\n",
    "# compile documents\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and processing\n",
    "\n",
    "### Stop Words\n",
    "\n",
    "> Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called *stop words*. - [Dropping common terms: stop words](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html) section in [Introduction to Information Retrevial](https://nlp.stanford.edu/IR-book/information-retrieval-book.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# install the stopwords corpus from the command line\n",
    "#!python -m nltk.downloader stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the `list` object to type [`set`](https://docs.python.org/3.7/library/stdtypes.html#set-types-set-frozenset) to speed up the testing of membership in future `if` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are transforming all words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sugar bad consume. sister likes sugar, father.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_free = \" \".join([word for word in doc1.lower().split(sep=\" \") if word not in stop])\n",
    "stop_free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclusion\n",
    "\n",
    "Punctuation marks - i.e. `!@#$%^&*(),;:'\"` - don't tell us anything about the words in a corpus. Removing them helps remove unnecessary characters from each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sugar bad consume sister likes sugar father'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc_free = \"\".join([char for char in stop_free if char not in exclude])\n",
    "punc_free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "> *Stemming* usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. *Lemmatization* usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the [*lemma*](https://simple.wikipedia.org/wiki/Lemma_(linguistics). If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. - [Stemming and Lemmatization](https://nlp.stanford.edu/IR-book/information-retrieval-book.html) section in [Introduction to Information Retrevial](https://nlp.stanford.edu/IR-book/information-retrieval-book.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# install the wordnet corpus from the command line\n",
    "#!python -m nltk.downloader wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sugar bad consume sister like sugar father'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put it all together in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(doc: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes in a raw string and returns normalized string.\n",
    "    \n",
    "    Normalization includes:\n",
    "     - lowercase spelling\n",
    "     - removing stopwords\n",
    "     - removing punctuation\n",
    "     - lemmatizing remaining words\n",
    "    \"\"\"\n",
    "    stop_free = \" \".join([word for word in doc.lower().split(sep=\" \") if word not in stop])\n",
    "    punc_free = ''.join(char for char in stop_free if char not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `normalize_text()` in a list comprehension to return a list of words belonging to each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sugar', 'bad', 'consume', 'sister', 'like', 'sugar', 'father'],\n",
       " ['father',\n",
       "  'spends',\n",
       "  'lot',\n",
       "  'time',\n",
       "  'driving',\n",
       "  'sister',\n",
       "  'around',\n",
       "  'dance',\n",
       "  'practice'],\n",
       " ['doctor',\n",
       "  'suggest',\n",
       "  'driving',\n",
       "  'may',\n",
       "  'cause',\n",
       "  'increased',\n",
       "  'stress',\n",
       "  'blood',\n",
       "  'pressure'],\n",
       " ['sometimes',\n",
       "  'feel',\n",
       "  'pressure',\n",
       "  'perform',\n",
       "  'well',\n",
       "  'school',\n",
       "  'father',\n",
       "  'never',\n",
       "  'seems',\n",
       "  'drive',\n",
       "  'sister',\n",
       "  'better'],\n",
       " ['health', 'expert', 'say', 'sugar', 'good', 'lifestyle']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean = [normalize_text(doc=doc).split(sep=\" \") for doc in doc_complete] \n",
    "doc_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Term Matrix\n",
    "\n",
    "Creating the term dictionary of our courpus, where every unique term is assigned an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(doc_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2)],\n",
       " [(2, 1), (4, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(8, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1)],\n",
       " [(2, 1),\n",
       "  (4, 1),\n",
       "  (18, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1)],\n",
       " [(5, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDA = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LDA(doc_term_matrix, \n",
    "            num_topics=3,\n",
    "            id2word=dictionary,\n",
    "            passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.071*\"father\" + 0.071*\"sister\" + 0.041*\"better\"'),\n",
       " (1, '0.050*\"driving\" + 0.050*\"pressure\" + 0.050*\"stress\"'),\n",
       " (2, '0.126*\"sugar\" + 0.071*\"like\" + 0.071*\"bad\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics(num_words=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line is a topic with individual topic terms and weights. \n",
    "\n",
    "* Topic 0 can be termed as stress;\n",
    "* Topic 1 can be termed as time spent driving; and\n",
    "* Topic 3 can be termed as family health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "LDA can be used to obtain topics from documents. To improve the LDA model, some future work may include any of the following:\n",
    "\n",
    "* Frequency filtering;\n",
    "* Part of Speech (POS) filtering; and\n",
    "* Batch wise LDA"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ENTER]",
   "language": "python",
   "name": "conda-env-ENTER-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
