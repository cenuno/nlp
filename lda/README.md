# Latent Dirichlent Allocation (LDA)

> In [natural language processing (NLP)](https://en.wikipedia.org/wiki/Natural_language_processing), [latent Dirichlet allocation (LDA)](http://jmlr.csail.mit.edu/papers/v3/blei03a.html) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, [if observations are words collected into documents](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168328/), it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. [LDA](https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation) is an example of a [topic model](https://cacm.acm.org/magazines/2012/4/147361-probabilistic-topic-models/fulltext#F3). - Wikipedia [source](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)

*Hyperlinks are a blend of my own and those found on the Wikipedia page for LDA*

